{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import pi\n",
    "from itertools import chain\n",
    "import os\n",
    "import sys\n",
    "dir_name = os.getcwd()\n",
    "parent_dir_name = os.path.dirname(dir_name)\n",
    "sys.path.insert(0, parent_dir_name)\n",
    "from modules.model_gpt import GPT, GPTConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import colormaps\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.text import Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix state dict '_orig_mode.' prefix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_state_dict_prefix(state_dict, prefix='_orig_mod.'):\n",
    "    new_state_dict = state_dict.copy()\n",
    "    for (key, value) in state_dict.items():\n",
    "        if key.startswith(prefix):\n",
    "            del new_state_dict[key]\n",
    "            new_state_dict.update({key[len(prefix):]: value})\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CausalSelfAttention model from weights: /Users/leroux/sEMG/saved_models/gpt2.pt\n",
      "number of parameters: 123.65M\n",
      "Initializing CausalSelfAttention model from weights: /Users/leroux/sEMG/saved_models/gpt2-from-scratch.pt\n",
      "number of parameters: 123.59M\n"
     ]
    }
   ],
   "source": [
    "device_linear = 'cuda:0'\n",
    "lr = 1e-3\n",
    "override_args = dict(           \n",
    "                   batch_size=32,\n",
    "                    # quantization_levels_input=16,\n",
    "                    # quantization_levels_weights=8,    \n",
    "                    # quantization_levels_output=32,                \n",
    "                   )\n",
    "\n",
    "# First model\n",
    "# init_from = \"/Data/pgi-15/common_models/dram_attention_project/gpt2-LinearDRAMAttention.pt\"\n",
    "init_from = \"/Users/leroux/sEMG/saved_models/gpt2.pt\"\n",
    "model_sd_linear = torch.load(init_from, map_location='cpu')\n",
    "model_ld_linear = model_sd_linear['model']\n",
    "model_ld_linear = remove_state_dict_prefix(model_ld_linear)\n",
    "config_args_linear = model_sd_linear['model_args']\n",
    "[config_args_linear.update({k:v}) for (k, v) in override_args.items()]\n",
    "print(f\"Initializing {config_args_linear['attention']} model from weights: {init_from}\")\n",
    "config = GPTConfig(**config_args_linear)\n",
    "model_linear = GPT(config)\n",
    "model_linear.load_state_dict(model_ld_linear, strict=False)\n",
    "\n",
    "# Model to compare\n",
    "device = 'cuda:1'\n",
    "init_from = \"/Users/leroux/sEMG/saved_models/gpt2-from-scratch.pt\"\n",
    "model_sd = torch.load(init_from, map_location='cpu')\n",
    "model_ld = model_sd['model']\n",
    "model_ld = remove_state_dict_prefix(model_ld)\n",
    "config_args = model_sd['model_args']\n",
    "override_args = dict(           \n",
    "                   batch_size=32,\n",
    "                    # attention=\"DRAMAttention\",             \n",
    "                   )\n",
    "[config_args.update({k:v}) for (k, v) in override_args.items()]\n",
    "print(f\"Initializing {config_args['attention']} model from weights: {init_from}\")\n",
    "config = GPTConfig(**config_args)\n",
    "model = GPT(config)\n",
    "model.load_state_dict(model_ld, strict=False)\n",
    "\n",
    "# [print(f\"Buffer {name}: {buf.size()}\") for name, buf in model.named_buffers()]\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Data Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_linear = torch.nn.DataParallel(model_linear, device_ids=[0,1,2,3]).to('cuda:0')\n",
    "# model = torch.nn.DataParallel(model, device_ids=[0,1,2,3]).to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Web Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('/Users/leroux/sEMG/datasets/texts/', \"openwebtext\")\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "block_size = config_args['block_size']\n",
    "batch_size = config_args['batch_size']\n",
    "device_type = 'cuda'\n",
    "\n",
    "block_size = 1024\n",
    "\n",
    "def get_batch(split, device_id):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device_id, non_blocking=True), y.pin_memory().to(device_id, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device_id), y.to(device_id)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix issue with model statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     model_linear.train()\n",
    "#     model_linear.to(device)\n",
    "#     for i in range(1000):\n",
    "#         X, Y = get_batch('train')     \n",
    "#         logits, loss = model_linear(X, Y)\n",
    "#         print(f'iter: {i}, loss: {loss.item():.2f}')\n",
    "\n",
    "# model_ld_linear = model_linear.state_dict()\n",
    "# model_sd_linear.update({'model': model_ld_linear})\n",
    "# del model_sd['optimizer']\n",
    "# torch.save(model_sd, \"/Users/leroux/sEMG/saved_models/gpt2-xl-LinearDRAMAttention-no-saved_optimizer.pt\")\n",
    "# print('ok')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init a and b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    X_linear, Y_linear = get_batch('train', device_linear)  \n",
    "    model_linear = model_linear.to(device_linear)\n",
    "    X, Y = get_batch('train', device)  \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    done = False\n",
    "    error_threshold = 0.001    \n",
    "    error_threshold_TIA = 0.2\n",
    "    max_calibration_iter = 100\n",
    "    alpha_max = 1.0\n",
    "    alpha_min = 0.01\n",
    "    alpha_decay_max_step = 50\n",
    "    # alpha_calibration = torch.cos(torch.arange(alpha_decay_max_step) / alpha_decay_max_step * pi / 2) + alpha_min\n",
    "    alpha_calibration = torch.linspace(alpha_max, alpha_min, alpha_decay_max_step)\n",
    "    cmap = colormaps['plasma']\n",
    "    # Take colors at regular intervals spanning the colormap.\n",
    "    colors = cmap(np.linspace(0, 1, 5 * config_args['n_layer']))\n",
    "    # colors = cmap(np.linspace(0, 1, 6 * config_args['n_layer']))\n",
    "\n",
    "    calibration_iter = 0\n",
    "    with torch.no_grad():        \n",
    "        for layer in model.transformer.h:\n",
    "        # for layer in model.module.transformer.h:\n",
    "            for scaler_a_b in [layer.attn.q_scaler, layer.attn.k_scaler, layer.attn.v_scaler, layer.attn.att_score_scaler, layer.attn.output_scaler]:\n",
    "            # for scaler_a_b in [layer.attn.q_scaler, layer.attn.k_scaler, layer.attn.v_scaler, layer.attn.att_score_scaler, layer.attn.NL_scaler, layer.attn.output_scaler]:\n",
    "                nn.init.constant_(scaler_a_b.a, val=1.0)\n",
    "                nn.init.constant_(scaler_a_b.b, val=0.0)\n",
    "        \n",
    "        std_errors_plots = torch.zeros(5 * config_args['n_layer'], 0)\n",
    "        mean_errors_plots = torch.zeros(5 * config_args['n_layer'], 0)\n",
    "        # std_errors_plots = torch.zeros(6 * config_args['n_layer'], 0)\n",
    "        # mean_errors_plots = torch.zeros(6 * config_args['n_layer'], 0)\n",
    "        losses = []\n",
    "        while not(done):\n",
    "            # X, Y = get_batch('train')\n",
    "            logits_linear, loss_linear = model_linear(X_linear, targets=Y_linear)\n",
    "            logits, loss = model(X, targets=Y)\n",
    "            if calibration_iter==0:\n",
    "                q = model.transformer.h[-1].attn.Q[0,:,0].flatten().cpu().clone()\n",
    "                q_after_scale = model.transformer.h[-1].attn.Q_after_scale[0,:,0].flatten().cpu().clone()\n",
    "            losses += [loss.item()]\n",
    "            # Init a and b w.r.t computed statistics\n",
    "            std_errors = []\n",
    "            mean_errors = []            \n",
    "            if calibration_iter < alpha_decay_max_step:\n",
    "                alpha = alpha_calibration[calibration_iter].item()\n",
    "            else:\n",
    "                alpha = alpha_min                \n",
    "            done_list = []\n",
    "            for l, layer in enumerate(model.transformer.h):\n",
    "                for param_id, scaler_a_b in enumerate([layer.attn.q_scaler, layer.attn.k_scaler, layer.attn.v_scaler, layer.attn.att_score_scaler, layer.attn.output_scaler]):\n",
    "                # for param_id, scaler_a_b in enumerate([layer.attn.q_scaler, layer.attn.k_scaler, layer.attn.v_scaler, layer.attn.att_score_scaler, layer.attn.NL_scaler, layer.attn.output_scaler]):\n",
    "                    std_errors += [torch.abs(scaler_a_b.std_after_scale-scaler_a_b.target_std).item()]   \n",
    "                    if param_id != 3:   \n",
    "                        threshold = error_threshold\n",
    "                    else:\n",
    "                        threshold = error_threshold_TIA\n",
    "                    if std_errors[-1] > threshold:\n",
    "                        if scaler_a_b.std_after_scale != 0.:\n",
    "                            new_a = scaler_a_b.a * scaler_a_b.target_std / scaler_a_b.std_after_scale\n",
    "                            scaler_a_b.a.fill_(alpha * new_a + (1-alpha) * scaler_a_b.a)\n",
    "                        done_list += [False]\n",
    "                    else:\n",
    "                        done_list += [True]\n",
    "                    \n",
    "                    mean_errors += [torch.abs(scaler_a_b.mean_after_scale-scaler_a_b.target_mean).item()]\n",
    "                    if param_id != 3: \n",
    "                        if mean_errors[-1] > error_threshold:\n",
    "                            new_b = scaler_a_b.b + (scaler_a_b.target_mean - scaler_a_b.mean_after_scale)\n",
    "                            scaler_a_b.b.fill_(alpha * new_b + (1-alpha) * scaler_a_b.b)\n",
    "                            done_list += [False]\n",
    "                        else:\n",
    "                            done_list += [True]\n",
    "                    else:\n",
    "                        done_list += [True]\n",
    "                    \n",
    "            std_errors_plots = torch.cat((std_errors_plots, torch.tensor(std_errors).unsqueeze(1)), dim=-1)\n",
    "            mean_errors_plots = torch.cat((mean_errors_plots, torch.tensor(mean_errors).unsqueeze(1)), dim=-1)\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "            clear_output(wait=True)\n",
    "            for p, param in enumerate(range(len(std_errors_plots))):\n",
    "                ax[0].plot(std_errors_plots[param], linewidth=0.5, color=colors[p])\n",
    "                ax[0].plot(mean_errors_plots[param], '--', linewidth=0.5, color=colors[p])\n",
    "            ax[0].set_yscale('log')\n",
    "            ax[0].set_xlabel('iters')\n",
    "            ax[0].hlines(error_threshold, 0., len(std_errors_plots[param])-1.0, colors='black', linewidth=2)\n",
    "            ax[1].plot(torch.tensor(losses), linewidth=1)\n",
    "            ax[1].set_xlabel('iters')\n",
    "            ax[1].set_ylabel('Cross Entropy Loss')\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            sys.stdout.flush()\n",
    "            plt.pause(0.1)  # Pause to update the plot\n",
    "            print(f'Calibraton iter {calibration_iter} | Loss: {loss.item():.4f} | error threshold: {error_threshold:.3f}\\tnum valid params: {torch.sum(torch.tensor(done_list))}/{len(done_list)}\\tstd errors: {torch.sort(torch.tensor(std_errors), descending=True)[0][:3]}\\tmean errors: {torch.sort(torch.tensor(mean_errors), descending=True)[0][:3]}')\n",
    "            calibration_iter += 1\n",
    "            assert calibration_iter < max_calibration_iter, f'Calibration algorithm did not converge after {calibration_iter} steps.'\n",
    "            if torch.all(torch.tensor(done_list)):\n",
    "                print(f'Calibration finished after {calibration_iter} steps.')\n",
    "                done = True          \n",
    "    print(f\"Losses tensor: {torch.tensor(losses)}\")\n",
    "    # End calibration procedure\n",
    "    # for layer in model.module.transformer.h:\n",
    "    for layer in model.transformer.h:\n",
    "        for scaler_a_b in [layer.attn.q_scaler, layer.attn.k_scaler, layer.attn.v_scaler, layer.attn.att_score_scaler, layer.attn.output_scaler]:\n",
    "        # for scaler_a_b in [layer.attn.q_scaler, layer.attn.k_scaler, layer.attn.v_scaler, layer.attn.att_score_scaler, layer.attn.NL_scaler, layer.attn.output_scaler]:\n",
    "            scaler_a_b.calibration = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch iteration: 0\tLinear loss: 3.15\tNonlinear loss: 3.09\tppl linear: 23.32\tppl: 21.95\n",
      "Batch iteration: 1\tLinear loss: 3.05\tNonlinear loss: 3.11\tppl linear: 21.04\tppl: 22.37\n",
      "Batch iteration: 2\tLinear loss: 3.13\tNonlinear loss: 3.26\tppl linear: 22.94\tppl: 25.98\n",
      "Batch iteration: 3\tLinear loss: 3.14\tNonlinear loss: 3.11\tppl linear: 23.07\tppl: 22.38\n",
      "Batch iteration: 4\tLinear loss: 3.11\tNonlinear loss: 2.99\tppl linear: 22.35\tppl: 19.79\n",
      "Batch iteration: 5\tLinear loss: 3.16\tNonlinear loss: 3.15\tppl linear: 23.47\tppl: 23.41\n",
      "Batch iteration: 6\tLinear loss: 3.05\tNonlinear loss: 3.19\tppl linear: 21.14\tppl: 24.40\n",
      "Batch iteration: 7\tLinear loss: 3.01\tNonlinear loss: 2.99\tppl linear: 20.35\tppl: 19.91\n",
      "Batch iteration: 8\tLinear loss: 3.11\tNonlinear loss: 3.27\tppl linear: 22.50\tppl: 26.25\n",
      "Batch iteration: 9\tLinear loss: 3.03\tNonlinear loss: 3.04\tppl linear: 20.61\tppl: 20.96\n",
      "Batch iteration: 10\tLinear loss: 3.01\tNonlinear loss: 3.22\tppl linear: 20.25\tppl: 25.14\n",
      "Batch iteration: 11\tLinear loss: 3.19\tNonlinear loss: 3.02\tppl linear: 24.18\tppl: 20.52\n",
      "Batch iteration: 12\tLinear loss: 3.04\tNonlinear loss: 3.17\tppl linear: 20.96\tppl: 23.89\n",
      "Batch iteration: 13\tLinear loss: 3.16\tNonlinear loss: 3.06\tppl linear: 23.48\tppl: 21.31\n",
      "Batch iteration: 14\tLinear loss: 3.10\tNonlinear loss: 3.06\tppl linear: 22.26\tppl: 21.22\n",
      "Batch iteration: 15\tLinear loss: 3.17\tNonlinear loss: 3.19\tppl linear: 23.80\tppl: 24.38\n",
      "Batch iteration: 16\tLinear loss: 3.15\tNonlinear loss: 3.16\tppl linear: 23.25\tppl: 23.53\n",
      "Batch iteration: 17\tLinear loss: 3.11\tNonlinear loss: 3.06\tppl linear: 22.37\tppl: 21.30\n",
      "Batch iteration: 18\tLinear loss: 3.17\tNonlinear loss: 3.15\tppl linear: 23.73\tppl: 23.34\n",
      "Batch iteration: 19\tLinear loss: 3.09\tNonlinear loss: 3.19\tppl linear: 22.06\tppl: 24.32\n",
      "Batch iteration: 20\tLinear loss: 3.09\tNonlinear loss: 3.09\tppl linear: 22.03\tppl: 22.06\n",
      "Batch iteration: 21\tLinear loss: 3.09\tNonlinear loss: 3.18\tppl linear: 21.99\tppl: 24.10\n",
      "Batch iteration: 22\tLinear loss: 3.09\tNonlinear loss: 3.12\tppl linear: 22.01\tppl: 22.63\n",
      "Batch iteration: 23\tLinear loss: 3.16\tNonlinear loss: 3.13\tppl linear: 23.56\tppl: 22.92\n",
      "Batch iteration: 24\tLinear loss: 3.05\tNonlinear loss: 3.16\tppl linear: 21.09\tppl: 23.58\n",
      "Batch iteration: 25\tLinear loss: 3.12\tNonlinear loss: 3.06\tppl linear: 22.61\tppl: 21.35\n",
      "Batch iteration: 26\tLinear loss: 3.14\tNonlinear loss: 3.21\tppl linear: 23.07\tppl: 24.78\n",
      "Batch iteration: 27\tLinear loss: 3.17\tNonlinear loss: 3.33\tppl linear: 23.76\tppl: 27.81\n",
      "Batch iteration: 28\tLinear loss: 3.15\tNonlinear loss: 3.11\tppl linear: 23.31\tppl: 22.42\n",
      "Batch iteration: 29\tLinear loss: 3.11\tNonlinear loss: 3.10\tppl linear: 22.49\tppl: 22.19\n",
      "Batch iteration: 30\tLinear loss: 3.19\tNonlinear loss: 3.15\tppl linear: 24.39\tppl: 23.23\n",
      "Batch iteration: 31\tLinear loss: 3.02\tNonlinear loss: 3.21\tppl linear: 20.58\tppl: 24.71\n",
      "Batch iteration: 32\tLinear loss: 3.02\tNonlinear loss: 3.20\tppl linear: 20.47\tppl: 24.45\n",
      "Batch iteration: 33\tLinear loss: 3.03\tNonlinear loss: 3.09\tppl linear: 20.71\tppl: 22.06\n",
      "Batch iteration: 34\tLinear loss: 3.03\tNonlinear loss: 3.20\tppl linear: 20.66\tppl: 24.42\n",
      "Batch iteration: 35\tLinear loss: 3.15\tNonlinear loss: 3.21\tppl linear: 23.36\tppl: 24.87\n",
      "Batch iteration: 36\tLinear loss: 3.14\tNonlinear loss: 3.10\tppl linear: 23.05\tppl: 22.24\n",
      "Batch iteration: 37\tLinear loss: 3.12\tNonlinear loss: 3.16\tppl linear: 22.59\tppl: 23.54\n",
      "Batch iteration: 38\tLinear loss: 3.05\tNonlinear loss: 3.09\tppl linear: 21.04\tppl: 21.87\n",
      "Batch iteration: 39\tLinear loss: 3.07\tNonlinear loss: 3.17\tppl linear: 21.44\tppl: 23.87\n",
      "Batch iteration: 40\tLinear loss: 3.11\tNonlinear loss: 3.21\tppl linear: 22.42\tppl: 24.88\n",
      "Batch iteration: 41\tLinear loss: 3.18\tNonlinear loss: 3.21\tppl linear: 23.99\tppl: 24.80\n",
      "Batch iteration: 42\tLinear loss: 3.20\tNonlinear loss: 3.22\tppl linear: 24.49\tppl: 25.14\n",
      "Batch iteration: 43\tLinear loss: 3.14\tNonlinear loss: 3.15\tppl linear: 23.16\tppl: 23.43\n",
      "Batch iteration: 44\tLinear loss: 3.00\tNonlinear loss: 3.02\tppl linear: 20.09\tppl: 20.46\n",
      "Batch iteration: 45\tLinear loss: 3.10\tNonlinear loss: 2.99\tppl linear: 22.22\tppl: 19.91\n",
      "Batch iteration: 46\tLinear loss: 3.10\tNonlinear loss: 3.24\tppl linear: 22.27\tppl: 25.56\n",
      "Batch iteration: 47\tLinear loss: 3.07\tNonlinear loss: 3.20\tppl linear: 21.54\tppl: 24.47\n",
      "Batch iteration: 48\tLinear loss: 3.15\tNonlinear loss: 3.13\tppl linear: 23.27\tppl: 22.86\n",
      "Batch iteration: 49\tLinear loss: 3.06\tNonlinear loss: 3.04\tppl linear: 21.40\tppl: 20.90\n",
      "Batch iteration: 50\tLinear loss: 3.09\tNonlinear loss: 3.27\tppl linear: 22.07\tppl: 26.35\n",
      "Batch iteration: 51\tLinear loss: 3.21\tNonlinear loss: 3.08\tppl linear: 24.74\tppl: 21.81\n",
      "Batch iteration: 52\tLinear loss: 3.18\tNonlinear loss: 3.09\tppl linear: 24.09\tppl: 21.94\n",
      "Batch iteration: 53\tLinear loss: 3.10\tNonlinear loss: 3.13\tppl linear: 22.09\tppl: 22.79\n",
      "Batch iteration: 54\tLinear loss: 3.08\tNonlinear loss: 3.27\tppl linear: 21.80\tppl: 26.37\n",
      "Batch iteration: 55\tLinear loss: 3.07\tNonlinear loss: 3.18\tppl linear: 21.57\tppl: 24.09\n",
      "Batch iteration: 56\tLinear loss: 3.07\tNonlinear loss: 3.18\tppl linear: 21.45\tppl: 23.98\n",
      "Batch iteration: 57\tLinear loss: 3.17\tNonlinear loss: 3.06\tppl linear: 23.76\tppl: 21.28\n",
      "Batch iteration: 58\tLinear loss: 3.13\tNonlinear loss: 3.21\tppl linear: 22.84\tppl: 24.88\n",
      "Batch iteration: 59\tLinear loss: 3.13\tNonlinear loss: 3.03\tppl linear: 22.86\tppl: 20.64\n",
      "Batch iteration: 60\tLinear loss: 3.10\tNonlinear loss: 3.13\tppl linear: 22.30\tppl: 22.93\n",
      "Batch iteration: 61\tLinear loss: 3.02\tNonlinear loss: 3.10\tppl linear: 20.57\tppl: 22.30\n",
      "Batch iteration: 62\tLinear loss: 3.12\tNonlinear loss: 3.10\tppl linear: 22.66\tppl: 22.12\n",
      "Batch iteration: 63\tLinear loss: 3.10\tNonlinear loss: 3.17\tppl linear: 22.22\tppl: 23.70\n",
      "Batch iteration: 64\tLinear loss: 3.10\tNonlinear loss: 3.15\tppl linear: 22.13\tppl: 23.31\n",
      "Batch iteration: 65\tLinear loss: 3.05\tNonlinear loss: 3.10\tppl linear: 21.09\tppl: 22.26\n",
      "Batch iteration: 66\tLinear loss: 3.08\tNonlinear loss: 3.20\tppl linear: 21.75\tppl: 24.64\n",
      "Batch iteration: 67\tLinear loss: 3.07\tNonlinear loss: 3.15\tppl linear: 21.52\tppl: 23.44\n",
      "Batch iteration: 68\tLinear loss: 3.18\tNonlinear loss: 3.06\tppl linear: 24.01\tppl: 21.37\n",
      "Batch iteration: 69\tLinear loss: 3.12\tNonlinear loss: 3.01\tppl linear: 22.68\tppl: 20.31\n",
      "Batch iteration: 70\tLinear loss: 3.11\tNonlinear loss: 3.33\tppl linear: 22.53\tppl: 27.99\n",
      "Batch iteration: 71\tLinear loss: 3.18\tNonlinear loss: 3.08\tppl linear: 24.06\tppl: 21.78\n",
      "Batch iteration: 72\tLinear loss: 3.09\tNonlinear loss: 3.12\tppl linear: 21.99\tppl: 22.60\n",
      "Batch iteration: 73\tLinear loss: 3.06\tNonlinear loss: 3.13\tppl linear: 21.25\tppl: 22.87\n",
      "Batch iteration: 74\tLinear loss: 3.20\tNonlinear loss: 3.14\tppl linear: 24.57\tppl: 23.10\n",
      "Batch iteration: 75\tLinear loss: 3.13\tNonlinear loss: 3.13\tppl linear: 22.80\tppl: 22.95\n",
      "Batch iteration: 76\tLinear loss: 3.15\tNonlinear loss: 3.14\tppl linear: 23.35\tppl: 23.09\n",
      "Batch iteration: 77\tLinear loss: 3.12\tNonlinear loss: 3.13\tppl linear: 22.72\tppl: 22.97\n",
      "Batch iteration: 78\tLinear loss: 3.05\tNonlinear loss: 3.12\tppl linear: 21.13\tppl: 22.68\n",
      "Batch iteration: 79\tLinear loss: 3.14\tNonlinear loss: 3.12\tppl linear: 23.00\tppl: 22.59\n",
      "Batch iteration: 80\tLinear loss: 3.07\tNonlinear loss: 3.11\tppl linear: 21.61\tppl: 22.44\n",
      "Batch iteration: 81\tLinear loss: 3.12\tNonlinear loss: 3.06\tppl linear: 22.65\tppl: 21.41\n",
      "Batch iteration: 82\tLinear loss: 3.10\tNonlinear loss: 3.17\tppl linear: 22.17\tppl: 23.70\n",
      "Batch iteration: 83\tLinear loss: 3.13\tNonlinear loss: 3.17\tppl linear: 22.98\tppl: 23.88\n",
      "Batch iteration: 84\tLinear loss: 3.10\tNonlinear loss: 3.25\tppl linear: 22.16\tppl: 25.68\n",
      "Batch iteration: 85\tLinear loss: 3.17\tNonlinear loss: 3.31\tppl linear: 23.79\tppl: 27.52\n",
      "Batch iteration: 86\tLinear loss: 3.09\tNonlinear loss: 3.23\tppl linear: 21.97\tppl: 25.20\n",
      "Batch iteration: 87\tLinear loss: 3.06\tNonlinear loss: 3.15\tppl linear: 21.42\tppl: 23.25\n",
      "Batch iteration: 88\tLinear loss: 3.03\tNonlinear loss: 3.12\tppl linear: 20.68\tppl: 22.62\n",
      "Batch iteration: 89\tLinear loss: 2.99\tNonlinear loss: 3.15\tppl linear: 19.83\tppl: 23.34\n",
      "Batch iteration: 90\tLinear loss: 3.07\tNonlinear loss: 3.09\tppl linear: 21.52\tppl: 21.93\n",
      "Batch iteration: 91\tLinear loss: 3.01\tNonlinear loss: 3.19\tppl linear: 20.20\tppl: 24.36\n",
      "Batch iteration: 92\tLinear loss: 2.98\tNonlinear loss: 3.16\tppl linear: 19.69\tppl: 23.55\n",
      "Batch iteration: 93\tLinear loss: 3.15\tNonlinear loss: 3.17\tppl linear: 23.40\tppl: 23.88\n",
      "Batch iteration: 94\tLinear loss: 3.16\tNonlinear loss: 3.14\tppl linear: 23.59\tppl: 23.02\n",
      "Batch iteration: 95\tLinear loss: 3.08\tNonlinear loss: 3.22\tppl linear: 21.70\tppl: 24.98\n",
      "Batch iteration: 96\tLinear loss: 3.12\tNonlinear loss: 3.25\tppl linear: 22.57\tppl: 25.84\n",
      "Batch iteration: 97\tLinear loss: 3.24\tNonlinear loss: 3.14\tppl linear: 25.43\tppl: 22.99\n",
      "Batch iteration: 98\tLinear loss: 3.06\tNonlinear loss: 3.13\tppl linear: 21.31\tppl: 22.95\n",
      "Batch iteration: 99\tLinear loss: 2.95\tNonlinear loss: 3.09\tppl linear: 19.09\tppl: 21.96\n",
      "Average: Linear loss: 3.10\tNonlinear loss: 3.14\tppl linear: 22.30\tppl: 23.25\n"
     ]
    }
   ],
   "source": [
    "model_linear = model_linear.to(device_linear)\n",
    "model = model.to(device)\n",
    "model_linear.eval()\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "ppl_linear = Perplexity().to(device_linear)\n",
    "ppl = Perplexity().to(device)\n",
    "\n",
    "avg_loss_linear = []\n",
    "avg_loss = []\n",
    "avg_ppl_linear = []\n",
    "avg_ppl = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(100):   \n",
    "        X_linear, Y_linear = get_batch('test', device_linear)     \n",
    "        logits_linear, loss_linear = model_linear(X_linear, targets=Y_linear)\n",
    "        X, Y = get_batch('test', device)  \n",
    "        logits, loss = model(X, targets=Y)\n",
    "        layer = 0        \n",
    "        # a = model_linear.transformer.h[layer].attn.x_cache.std()\n",
    "        # b = model_linear.transformer.h[layer].attn.x_cache.mean()\n",
    "        # c = model.transformer.h[layer].attn.x_cache.std()\n",
    "        # d = model.transformer.h[layer].attn.x_cache.mean()\n",
    "        # print(f'Batch iteration: {i}\\tLinear loss: {loss_linear:.2f}\\tNonlinear loss: {loss:.2f}\\t Linear model std mean: {a:.3f}, {b:.3f}\\t Nonlinear model std mean: {c:.3f}, {d:.3f}')\n",
    "        \n",
    "        # ppl_exp = torch.exp(loss) -> same measure as perplexity\n",
    "        ppl_linear_model = ppl_linear(logits_linear, Y_linear)\n",
    "        ppl_model = ppl(logits, Y)\n",
    "        \n",
    "        avg_loss_linear += [loss_linear.item()]\n",
    "        avg_loss += [loss.item()]\n",
    "        avg_ppl_linear += [ppl_linear_model.item()]\n",
    "        avg_ppl += [ppl_model.item()]\n",
    "        \n",
    "        print(f'Batch iteration: {i}\\tLinear loss: {loss_linear:.2f}\\tNonlinear loss: {loss:.2f}\\tppl linear: {ppl_linear_model:.2f}\\tppl: {ppl_model:.2f}')\n",
    "\n",
    "avg_loss_linear = np.array(avg_loss_linear).mean()\n",
    "avg_loss = np.array(avg_loss).mean()\n",
    "avg_ppl_linear = np.array(avg_ppl_linear).mean()\n",
    "avg_ppl = np.array(avg_ppl).mean()\n",
    "\n",
    "print(f'Average: Linear loss: {avg_loss_linear:.2f}\\tNonlinear loss: {avg_loss:.2f}\\tppl linear: {avg_ppl_linear:.2f}\\tppl: {avg_ppl:.2f}')\n",
    "\n",
    "# torch.save({'model': model_linear.state_dict()}, '/Users/leroux/sEMG/saved_models/gpt2_with_scaling_statistics.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits, loss = model(X, targets=Y)\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15/2.54, 10/2.54))\n",
    "q_linear = model_linear.transformer.h[-1].attn.Q[0,:,0].flatten().cpu().clone()\n",
    "q_linear_after_scale = model_linear.transformer.h[-1].attn.Q_after_scale[0,:,0].flatten().cpu().clone()\n",
    "q_after_adapation = model.transformer.h[-1].attn.Q[0,:,0].flatten().cpu().clone()\n",
    "q_after_adapation_after_scale = model.transformer.h[-1].attn.Q_after_scale[0,:,0].flatten().cpu().clone()\n",
    "rwidth = 0.8\n",
    "ax[0,0].hist(q_linear, rwidth=rwidth)\n",
    "ax[1,0].hist(q_linear_after_scale, rwidth=rwidth)\n",
    "ax[0,1].hist(q, rwidth=rwidth)\n",
    "ax[1,1].hist(q_after_scale, rwidth=rwidth)\n",
    "ax[0,2].hist(q_after_adapation, rwidth=rwidth)\n",
    "ax[1,2].hist(q_after_adapation_after_scale, rwidth=rwidth)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test multiples quantization (without quantization aware training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_batch('test')\n",
    "input_levels = torch.tensor([2**32, 64, 32, 16, 8, 4, 2])\n",
    "weights_levels = torch.tensor([2**32, 64, 32, 16, 8, 4, 2])\n",
    "# input_levels = torch.tensor([2**32, 64, 32, 16])\n",
    "# weights_levels = torch.tensor([2**32, 64, 32, 16])\n",
    "input_levels_len = len(input_levels)\n",
    "weights_levels_len = len(weights_levels)\n",
    "losses = torch.zeros(input_levels_len, weights_levels_len)\n",
    "losses_std = torch.zeros(input_levels_len, weights_levels_len)\n",
    "for i, input_level in enumerate(input_levels):\n",
    "    for j, weights_level in enumerate(weights_levels):\n",
    "        config_args.update({\"quantization_levels_input\": input_level})\n",
    "        config_args.update({\"quantization_levels_weights\": weights_level})\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        model.load_state_dict(model_ld, strict=False)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1, reduce=False)\n",
    "            loss = loss.view(config_args['batch_size'], config_args['block_size'])\n",
    "            loss = loss.mean(dim=-1) # average over tokens\n",
    "            loss_mean, loss_std = loss.mean(), loss.std()\n",
    "        losses[i, j] = loss_mean.item()\n",
    "        losses_std[i, j] = loss_std.item()\n",
    "        print(f'input levels: {input_level}\\tweights_level: {weights_level}\\tLoss: {loss_mean.item():.4f}\\tStandard dev: {loss_std.item():.4f}')\n",
    "\n",
    "print('losses mean', losses)\n",
    "print('losses std', losses_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved quantize experience with \"checkpoints/gpt2-DRAMAttention_temporal_encoding_real_grad_lr_1e-5/ckpt\" (output quantize, outpus clamp, decay not implemented yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_levels = torch.tensor([2**32, 64, 32, 16, 8, 4, 2])\n",
    "weights_levels = torch.tensor([2**32, 64, 32, 16, 8, 4, 2])\n",
    "\n",
    "losses = torch.tensor([[3.1784, 3.1790, 3.1806, 3.1893, 3.2325, 4.0268, 9.2421],\n",
    "        [3.1793, 3.1801, 3.1823, 3.1919, 3.2328, 4.0729, 9.3272],\n",
    "        [3.1813, 3.1823, 3.1826, 3.1950, 3.2400, 4.0452, 9.2603],\n",
    "        [3.2022, 3.2033, 3.2034, 3.2152, 3.2580, 4.1199, 9.1787],\n",
    "        [3.4788, 3.4855, 3.4964, 3.5085, 3.6436, 4.9188, 9.1064],\n",
    "        [5.4929, 5.4719, 5.4791, 5.5990, 6.0570, 7.7586, 8.8878],\n",
    "        [8.2598, 8.2439, 8.2380, 8.2542, 8.2821, 8.2791, 9.0375]])\n",
    "\n",
    "losses_std = torch.tensor([[0.3129, 0.3121, 0.3144, 0.3122, 0.3117, 0.2819, 0.4138],\n",
    "        [0.3136, 0.3135, 0.3140, 0.3088, 0.3106, 0.3500, 0.4447],\n",
    "        [0.3113, 0.3111, 0.3126, 0.3101, 0.3121, 0.3053, 0.5186],\n",
    "        [0.3115, 0.3132, 0.3124, 0.3110, 0.3075, 0.3048, 0.5548],\n",
    "        [0.2694, 0.2728, 0.2812, 0.2676, 0.2618, 0.2552, 0.6936],\n",
    "        [0.7377, 0.7313, 0.7468, 0.6983, 0.6816, 0.3085, 0.4240],\n",
    "        [0.2267, 0.2179, 0.2080, 0.2623, 0.1870, 0.1764, 0.1862]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 12\n",
    "# Errorbar plots\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(5)\n",
    "fig.set_figheight(3)\n",
    "weights_levels_ = weights_levels[1:-1]\n",
    "input_levels_ = input_levels[1:-1]\n",
    "losses_ = losses[1:-1]\n",
    "losses_ = losses_[:, 1:-1]\n",
    "losses_std_ = losses_std[1:-1]\n",
    "losses_std_ = losses_std_[:, 1:-1]\n",
    "cmap = colormaps['plasma']\n",
    "for i, input_level in enumerate(input_levels_):\n",
    "    ax.errorbar(torch.log2(weights_levels_), losses_[i], yerr=losses_std_[i], fmt='-o', linewidth=1, c=cmap(i/len(input_levels_)), label=f'{torch.log2(input_level).item():.0f} input bits')\n",
    "    ax.hlines(losses[i, 0], torch.log2(weights_levels_[0]), torch.log2(weights_levels_[-1]), linestyles='--', linewidth=1.0, color=cmap(i/len(input_levels_)),\n",
    "            #   label=f'{torch.log2(input_level).item():.0f} input bits reference',\n",
    "              )\n",
    "ax.set_xlabel('# KV bits', fontsize=fontsize)\n",
    "ax.set_ylabel('Cross-entropy loss', fontsize=fontsize)\n",
    "# ax.invert_xaxis()\n",
    "ax.set_yscale('log')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1.0), loc='upper right', frameon=False, fontsize=fontsize)\n",
    "plt.xticks([2, 3, 4, 5, 6])\n",
    "plt.yticks([3, 4, 5, 6, 7, 8])\n",
    "from matplotlib.ticker import StrMethodFormatter, NullFormatter\n",
    "ax.yaxis.set_major_formatter(StrMethodFormatter('{x:.0f}'))\n",
    "# ax.set_yscale('log')\n",
    "fig.tight_layout()\n",
    "\n",
    "file_out = '/Users/leroux/sEMG/python_codes/plots/KV_and_Q_quantization_results'\n",
    "for fmt in ['png', 'svg', 'pdf']:\n",
    "    plt.savefig(file_out + '.%s' % fmt, format=fmt, dpi=1200)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 3D plot\n",
    "input_levels_len = len(input_levels)\n",
    "weights_levels_len = len(weights_levels)\n",
    "\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.view_init(elev=30, azim=45, roll=0)\n",
    "\n",
    "# input_levels_ = input_levels[1:-1]\n",
    "# weights_levels_ = weights_levels[1:-1]\n",
    "# losses_ = losses[1:-1]\n",
    "# losses_ = losses_[:,1:-1]\n",
    "\n",
    "input_levels_ = input_levels[1:-2]\n",
    "weights_levels_ = weights_levels[1:-2]\n",
    "losses_ = losses[1:-2]\n",
    "losses_ = losses_[:,1:-2]\n",
    "\n",
    "# x_mesh, y_mesh = np.meshgrid(torch.log2(input_levels_), torch.log2(weights_levels_))\n",
    "x_mesh, y_mesh = torch.log2(input_levels_).unsqueeze(-1).expand(-1,len(weights_levels_)), torch.log2(weights_levels_).unsqueeze(0).expand(len(input_levels_),-1),\n",
    "x, y = x_mesh.ravel(), y_mesh.ravel()\n",
    "top = losses_.cpu().ravel()\n",
    "bottom = losses_.min().cpu() * torch.ones_like(top)\n",
    "width = depth = 1\n",
    "\n",
    "ax.bar3d(x, y, bottom, dx=1, dy=1, dz=top-bottom, shade=True)\n",
    "ax.set_xlabel('# input bits')\n",
    "ax.set_ylabel('# KV bits')\n",
    "ax.set_zlabel('Cross entropy loss')\n",
    "# ax.set_zscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save statistics histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In place histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    total_q = torch.zeros(0)\n",
    "    total_k = torch.zeros(0)\n",
    "    total_v = torch.zeros(0)\n",
    "    total_A = torch.zeros(0)\n",
    "    total_out = torch.zeros(0)\n",
    "    for layer in model.transformer.h:\n",
    "        total_q = torch.cat((total_q, layer.attn.bins_count_q.data), dim=0)\n",
    "        total_k = torch.cat((total_k, layer.attn.bins_count_k.data), dim=0)\n",
    "        total_v = torch.cat((total_v, layer.attn.bins_count_v.data), dim=0)\n",
    "        total_A = torch.cat((total_A, layer.attn.bins_count_A.data), dim=0)\n",
    "        total_out = torch.cat((total_out, layer.attn.bins_count_out.data), dim=0)\n",
    "        break\n",
    "    \n",
    "    total_density_q, bins_edges_q = torch.histogram(total_q, bins=config_args['quantization_levels_input'], density=False)\n",
    "    total_density_k, bins_edges_k = torch.histogram(total_k, bins=config_args['quantization_levels_weights'], density=False)\n",
    "    total_density_v, bins_edges_v = torch.histogram(total_v, bins=config_args['quantization_levels_weights'], density=False)\n",
    "    total_density_A, bins_edges_A = torch.histogram(total_A, bins=config_args['quantization_levels_input'], density=False)\n",
    "    total_density_out, bins_edges_out = torch.histogram(total_out, bins=config_args['quantization_levels_output'], density=False)\n",
    "    \n",
    "    total_density_q /= total_q.numel()\n",
    "    total_density_k /= total_k.numel()\n",
    "    total_density_v /= total_v.numel()\n",
    "    total_density_A /= total_A.numel()\n",
    "    total_density_out /= total_out.numel()\n",
    "\n",
    "    hist_to_save = {'Q': {'density': total_density_q.to(torch.float).numpy(), 'bins_edges': bins_edges_q[:-1].to(torch.float).numpy()},\n",
    "                    'K': {'density': total_density_k.to(torch.float).numpy(), 'bins_edges': bins_edges_k[:-1].to(torch.float).numpy()},\n",
    "                    'V': {'density': total_density_v.to(torch.float).numpy(), 'bins_edges': bins_edges_v[:-1].to(torch.float).numpy()},\n",
    "                    'Attention': {'density': total_density_A.to(torch.float).numpy(), 'bins_edges': bins_edges_A[:-1].to(torch.float).numpy()},\n",
    "                    'Output': {'density': total_density_out.to(torch.float).numpy(), 'bins_edges': bins_edges_out[:-1].to(torch.float).numpy()},\n",
    "    }\n",
    "    \n",
    "    file_name = f\"./{quantization}_levels_histogram.npz\"\n",
    "    np.savez(file_name, **hist_to_save, allow_pickle=True)\n",
    "    \n",
    "    network_histogram = np.load(file_name, allow_pickle=True)\n",
    "    for (key, value) in network_histogram.items():\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hist density plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# file_name = f\"./16_levels_histogram_temporal_encoding.npz\"\n",
    "# file_name = f\"./256_levels_histogram_saturated_layer_1.npz\"\n",
    "network_histogram = np.load(file_name, allow_pickle=True)\n",
    "fig, ax = plt.subplots(1, len(network_histogram) - 1)\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(2.5)\n",
    "fontsize=12\n",
    "names = ['Q', 'K', 'V', 'Attention', 'Output']\n",
    "for i, (key, param) in enumerate(network_histogram.items()):\n",
    "    if i==len(network_histogram)-1:\n",
    "        break\n",
    "    param = param[()]\n",
    "    if i==3:\n",
    "        pass\n",
    "    \n",
    "    ax[i].bar(param['bins_edges'], param['density'], color='darkblue', width=0.05) \n",
    "    ax[i].plot(param['bins_edges'], param['density'], color='darkblue', linewidth=2)         \n",
    "    ax[i].set_xlabel(names[i], fontsize=fontsize)\n",
    "    if i==0:\n",
    "        ax[i].set_ylabel('Frequency')  \n",
    "    ax[i].set_yscale('log')\n",
    "fig.tight_layout()\n",
    "file_out = '/Users/leroux/sEMG/python_codes/plots/values_density_log'\n",
    "# file_out = '/Users/leroux/sEMG/python_codes/plots/values_density'\n",
    "# for fmt in ['png', 'svg', 'pdf']:\n",
    "#     plt.savefig(file_out + '.%s' % fmt, format=fmt, dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_linear = torch.nn.functional.softmax(logits_linear, dim=-1)\n",
    "probability = torch.nn.functional.softmax(logits, dim=-1)\n",
    "n_samples, n_tokens, vocab_size = logits_linear.shape\n",
    "token = int(torch.rand(1)*n_tokens)\n",
    "target = torch.nn.functional.one_hot(Y, num_classes=vocab_size)\n",
    "xaxis = torch.arange(0, vocab_size)\n",
    "n_values = 200\n",
    "for i in range(n_samples):\n",
    "    fig, ax = plt.subplots()\n",
    "    clear_output(wait=True)\n",
    "    value = Y[i, token]\n",
    "    max_idx_linear = torch.argmax(probability_linear[i, token])\n",
    "    max_idx_other= torch.argmax(probability[i, token])\n",
    "    print(f'Target word: {value}\\t Linear model word: {max_idx_linear}\\t Other network word: {max_idx_other}') \n",
    "    ax.bar(xaxis[value-n_values:value+n_values], target[i, token, value-n_values:value+n_values].detach().cpu().numpy(), alpha=0.5, label='Target', color='blue')\n",
    "    ax.bar(xaxis[value-n_values:value+n_values], probability_linear[i, token, value-n_values:value+n_values].detach().cpu().numpy(), alpha=0.5, label='Linear out', color='red')\n",
    "    ax.bar(xaxis[value-n_values:value+n_values], probability[i, token, value-n_values:value+n_values].detach().cpu().numpy(), alpha=0.5, label='Normal network out', color='green')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title(f'Output probabillities')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    plt.show()\n",
    "    sys.stdout.flush()\n",
    "    plt.pause(1.0) # Pause to update the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram compare intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # for i in range(len(model_linear.transformer.h)):\n",
    "    for i in range(0):\n",
    "        out_linear = model_linear.transformer.h[i].attn.x_cache\n",
    "        out = model.transformer.h[i].attn.x_cache\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        clear_output(wait=True)\n",
    "        mask = model_linear.transformer.h[i].attn.masking_1.mask.expand(config_args['batch_size'], config_args['n_head'], -1, -1).flatten()!=0\n",
    "        mask = torch.ones_like(out, dtype=torch.bool).flatten()\n",
    "        ax[0].hist(\n",
    "                    out_linear.flatten()[mask].detach().cpu().numpy(),\n",
    "                    alpha=0.5,\n",
    "                    label='Linear out',\n",
    "                #    density=True,\n",
    "                #    bins='auto',\n",
    "                    bins=100,\n",
    "                )\n",
    "        ax[0].hist(\n",
    "                    out.flatten()[mask].detach().cpu().numpy(),\n",
    "                    alpha=0.5,\n",
    "                    label='Nonlinear out',\n",
    "                    # density=True,\n",
    "                    # bins='auto',\n",
    "                    bins=100,\n",
    "                )\n",
    "        ax[0].legend(loc='upper right')\n",
    "        ax[0].set_title(f'Distribution attention layer {i}')\n",
    "        ax[0].set_xlabel('Value')\n",
    "        ax[0].set_ylabel('Frequency')\n",
    "        \n",
    "        mask = model_linear.transformer.h[i].attn.masking_1.mask[0,0].flatten()!=0\n",
    "        mask = torch.ones_like(out[0,0], dtype=torch.bool).flatten()\n",
    "        ax[1].plot(out_linear[0,0].flatten()[mask].detach().cpu().numpy(), out_linear[0,0].flatten()[mask].detach().cpu().numpy(), 'k', linewidth=1, label='Linear vs linear')\n",
    "        ax[1].scatter(out_linear[0,0].flatten()[mask].detach().cpu().numpy(), out_linear[0,0].flatten()[mask].detach().cpu().numpy(), color='darkblue', s=1, label='Linear vs linear')\n",
    "        ax[1].scatter(out_linear[0,0].flatten()[mask].detach().cpu().numpy(), out[0,0].flatten()[mask].detach().cpu().numpy(), color='darkred', s=1, label='Nonlinear vs linear')\n",
    "        \n",
    "        # ax[1].scatter(out[0,0].flatten()[mask].detach().cpu().numpy(), out[0,0].flatten()[mask].detach().cpu().numpy(), color='darkblue', s=1, label='Nonlinear vs Nonlinear')\n",
    "        # ax[1].scatter(out[0,0].flatten()[mask].detach().cpu().numpy(), out_linear[0,0].flatten()[mask].detach().cpu().numpy(), color='darkred', s=1, label='Linear vs Nonlinear')\n",
    "        \n",
    "        ax[1].legend(loc='upper right')\n",
    "        ax[1].set_title(f'Attention layer {i}')\n",
    "        ax[1].set_xlabel('X')\n",
    "        ax[1].set_ylabel('Y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        sys.stdout.flush()\n",
    "        plt.pause(0.1) # Pause to update the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trye optimize through backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear = model_linear.to(device)\n",
    "model_linear.eval()\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "calib_lr = 1e-4\n",
    "max_iter = 10000\n",
    "alpha = 0.1\n",
    "\n",
    "loss_list = []\n",
    "loss_total = []\n",
    "loss = 1000.\n",
    "# parameters = []\n",
    "# for layer in model.transformer.h:\n",
    "#     parameters += list(layer.attn.q_scaler.parameters())\n",
    "#     parameters += list(layer.attn.k_scaler.parameters())\n",
    "#     parameters += list(layer.attn.v_scaler.parameters())\n",
    "#     parameters += list(layer.attn.att_score_scaler.parameters())\n",
    "#     parameters += list(layer.attn.output_scaler.parameters())\n",
    "parameters = model.parameters()\n",
    "optim = torch.optim.AdamW(params=parameters, lr=calib_lr)\n",
    "for iter in range(max_iter):        \n",
    "    optim.zero_grad()\n",
    "    X, Y = get_batch('train')\n",
    "    device = X.device\n",
    "    b, t = X.size()\n",
    "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "    # Linear model\n",
    "    with torch.no_grad():        \n",
    "        out_linear, cel_linear = model_linear(X, Y)\n",
    "    # Nonlinear model\n",
    "    out, cel = model(X, Y)\n",
    "    if iter==0:\n",
    "        loss_total += [cel.item()]\n",
    "    else:\n",
    "        loss_total += [cel.item()*alpha + loss_total[-1]*(1-alpha)]\n",
    "    loss = cel.clone()\n",
    "    # loss = nn.MSELoss()(out, out_linear)\n",
    "    # loss = 0.5 * cel + 0.5 * loss\n",
    "    loss_list += [loss.item()]\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(f'iter: {iter}\\tLoss: {loss.item():.4f}\\tRunning average loss: {loss_total[-1]:.4f}')\n",
    "    if iter%10==0:\n",
    "        fig, ax = plt.subplots(1,3)\n",
    "        clear_output(wait=True)\n",
    "        # ax[0].scatter(out_linear[0].flatten().detach().cpu().numpy(), out_linear[0].flatten().detach().cpu().numpy(), color='darkblue', s=1, label='Linear vs linear')\n",
    "        # ax[0].scatter(out_linear[0].flatten().detach().cpu().numpy(), out[0].flatten().detach().cpu().numpy(), color='darkred', s=1, label='Nonlinear vs linear')\n",
    "        ax[0].scatter(out_linear[0,0].flatten().detach().cpu().numpy(), out_linear[0,0].flatten().detach().cpu().numpy(), color='darkblue', s=1, label='Linear vs linear')\n",
    "        ax[0].scatter(out_linear[0,0].flatten().detach().cpu().numpy(), out[0,0].flatten().detach().cpu().numpy(), color='darkred', s=1, label='Nonlinear vs linear')\n",
    "        ax[1].plot(torch.tensor(loss_list), linewidth=1)\n",
    "        ax[1].set_yscale('log')\n",
    "        ax[1].set_ylabel('Loss')        \n",
    "        ax[2].plot(torch.tensor(loss_total), linewidth=1)\n",
    "        ax[2].set_ylabel('Global cross entropy loss')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        sys.stdout.flush()\n",
    "        plt.pause(0.1)  # Pause to update the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trye optimize through backprop layer-wize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear = model_linear.to(device)\n",
    "model_linear.eval()\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "calib_lr = 1e-4\n",
    "max_iter = 10000\n",
    "loss_threshold = 1e-2\n",
    "layer_num = -1\n",
    "\n",
    "loss_list = []\n",
    "loss_total = []\n",
    "loss = 1000.\n",
    "# parameters = []\n",
    "for iter in range(max_iter):\n",
    "    if loss < loss_threshold or iter==0:\n",
    "        layer_num += 1\n",
    "        if layer_num+1 > config_args['n_layer']:\n",
    "            break\n",
    "        layer = model.transformer.h[layer_num]\n",
    "        parameters = []\n",
    "        parameters += list(layer.attn.q_scaler.parameters())\n",
    "        parameters += list(layer.attn.k_scaler.parameters())\n",
    "        parameters += list(layer.attn.v_scaler.parameters())\n",
    "        parameters += list(layer.attn.att_score_scaler.parameters())\n",
    "        parameters += list(layer.attn.output_scaler.parameters())        \n",
    "        optim = torch.optim.AdamW(params=parameters, lr=calib_lr)\n",
    "        \n",
    "    optim.zero_grad()\n",
    "    X, Y = get_batch('train')\n",
    "    device = X.device\n",
    "    b, t = X.size()\n",
    "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Linear model\n",
    "        tok_emb = model_linear.transformer.wte(X) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = model_linear.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        out_linear = model_linear.transformer.drop(tok_emb + pos_emb)\n",
    "        for l in range(layer_num):\n",
    "            out_linear = model_linear.transformer.h[l](out_linear)\n",
    "        out_linear = model_linear.transformer.h[layer_num](out_linear)\n",
    "        # out_linear = model_linear.transformer.h[layer_num].attn(model_linear.transformer.h[layer_num].ln_1(out_linear))\n",
    "        # Nonlinear model\n",
    "        tok_emb = model.transformer.wte(X) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = model.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        out = model.transformer.drop(tok_emb + pos_emb)\n",
    "        for l in range(layer_num):\n",
    "            out = model.transformer.h[l](out)\n",
    "    out = model.transformer.h[layer_num](out)\n",
    "    # out = model.transformer.h[layer_num].attn(model.transformer.h[layer_num].ln_1(out))\n",
    "\n",
    "    loss = nn.MSELoss()(out, out_linear)\n",
    "    # loss = nn.CrossEntropyLoss()(torch.nn.functional.softmax(out, dim=-1), torch.nn.functional.softmax(out_linear, dim=-1))\n",
    "    loss_list += [loss.item()]\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(f'iter: {iter}\\tLoss: {loss.item():.4f}\\tLayer: {layer_num}')\n",
    "    if iter%10==0:\n",
    "        fig, ax = plt.subplots(1,3)\n",
    "        clear_output(wait=True)\n",
    "        ax[0].scatter(out_linear[0].flatten().detach().cpu().numpy(), out_linear[0].flatten().detach().cpu().numpy(), color='darkblue', s=1, label='Linear vs linear')\n",
    "        ax[0].scatter(out_linear[0].flatten().detach().cpu().numpy(), out[0].flatten().detach().cpu().numpy(), color='darkred', s=1, label='Nonlinear vs linear')\n",
    "        ax[1].plot(torch.tensor(loss_list), linewidth=1)\n",
    "        ax[1].set_yscale('log')\n",
    "        ax[1].set_ylabel('Cross model layer loss')\n",
    "        with torch.no_grad():\n",
    "            logits, loss_global = model(X, Y)\n",
    "        loss_total += [loss_global.item()]\n",
    "        ax[2].plot(torch.tensor(loss_total), linewidth=1)\n",
    "        ax[2].set_ylabel('Global cross entropy loss')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        sys.stdout.flush()\n",
    "        plt.pause(0.1)  # Pause to update the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration with stop updating after multiple valid values for specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    done = False\n",
    "    error_threshold = 0.001    \n",
    "    max_calibration_iter = 1000\n",
    "    alpha_min = 0.01\n",
    "    alpha_decay_max_step = 50\n",
    "    # alpha_calibration = torch.cos(torch.arange(alpha_decay_max_step) / alpha_decay_max_step * pi / 2) + alpha_min\n",
    "    alpha_calibration = torch.linspace(1, alpha_min, alpha_decay_max_step)\n",
    "    \n",
    "    calibration_iter = 0\n",
    "    with torch.no_grad():\n",
    "        std_errors_plots = torch.zeros(6 * config_args['n_layer'], 0)\n",
    "        mean_errors_plots = torch.zeros(6 * config_args['n_layer'], 0)\n",
    "        done_list = torch.zeros(6 * config_args['n_layer'] * 2, 0, dtype=torch.bool)\n",
    "        while not(done):\n",
    "            done_list = torch.cat((done_list, torch.zeros(72 * 2, dtype=torch.bool).unsqueeze(1)), dim=-1)\n",
    "            logits, loss = model(X, Y)\n",
    "            # Init a and b w.r.t computed statistics\n",
    "            std_errors = []\n",
    "            mean_errors = []            \n",
    "            if calibration_iter < alpha_decay_max_step:\n",
    "                alpha = alpha_calibration[calibration_iter].item()\n",
    "            else:\n",
    "                alpha = alpha_min\n",
    "                \n",
    "            param_id = 0\n",
    "            for layer in model.transformer.h:\n",
    "                for scaler_a_b in [layer.attn.q_scaler, layer.attn.k_scaler, layer.attn.v_scaler, layer.attn.att_score_scaler, layer.attn.NL_scaler, layer.attn.output_scaler]:\n",
    "                    \n",
    "                    std_errors += [torch.abs(scaler_a_b.std_after_scale-scaler_a_b.target_std).item()]                   \n",
    "                    if std_errors[-1] > error_threshold:\n",
    "                    # if (std_errors[-1] > error_threshold and done_list[param_id, :-10].sum()<10) or calibration_iter<10:\n",
    "                        new_a = scaler_a_b.a * scaler_a_b.target_std / scaler_a_b.std_after_scale\n",
    "                        scaler_a_b.a.fill_(alpha * new_a + (1-alpha) * scaler_a_b.a)\n",
    "                    else:\n",
    "                        done_list[param_id, -1] += True\n",
    "                    param_id += 1\n",
    "                    \n",
    "                    mean_errors += [torch.abs(scaler_a_b.mean_after_scale-scaler_a_b.target_mean).item()]\n",
    "                    if mean_errors[-1] > error_threshold:\n",
    "                    # if (mean_errors[-1] > error_threshold and done_list[param_id, :-10].sum()<10) or calibration_iter<10:\n",
    "                        new_b = scaler_a_b.b + (scaler_a_b.target_mean - scaler_a_b.mean_after_scale)\n",
    "                        scaler_a_b.b.fill_(alpha * new_b + (1-alpha) * scaler_a_b.b)\n",
    "                    else:\n",
    "                        done_list[param_id, -1] += True                       \n",
    "                    param_id += 1\n",
    "                    \n",
    "            std_errors_plots = torch.cat((std_errors_plots, torch.tensor(std_errors).unsqueeze(1)), dim=-1)\n",
    "            mean_errors_plots = torch.cat((mean_errors_plots, torch.tensor(mean_errors).unsqueeze(1)), dim=-1)\n",
    "            fig, ax = plt.subplots()\n",
    "            clear_output(wait=True)\n",
    "            for param in range(len(std_errors_plots)):\n",
    "                ax.plot(std_errors_plots[param], linewidth=0.5)\n",
    "                ax.plot(mean_errors_plots[param], linewidth=0.5)\n",
    "                ax.set_yscale('log')\n",
    "            ax.hlines(error_threshold, 0., len(std_errors_plots[param])-1.0, colors='black', linewidth=2)\n",
    "            plt.show()\n",
    "            sys.stdout.flush()\n",
    "            plt.pause(0.1)  # Pause to update the plot\n",
    "            print(f'Calibraton iter {calibration_iter} | error threshold: {error_threshold:.3f}\\tnum valid params: {torch.sum(torch.tensor(done_list[:, -1]))}/{len(done_list[:, -1])}\\tstd errors: {torch.sort(torch.tensor(std_errors), descending=True)[0][:3]}\\tmean errors: {torch.sort(torch.tensor(mean_errors), descending=True)[0][:3]}')\n",
    "            calibration_iter += 1\n",
    "            assert calibration_iter < max_calibration_iter, f'Calibration algorithm did not converge after {calibration_iter} steps.'\n",
    "            if torch.all(torch.tensor(done_list[:, -1])):\n",
    "                done = True            \n",
    "    # End calibration procedure\n",
    "    # for layer in model.module.transformer.h:\n",
    "    for layer in model.transformer.h:\n",
    "        for scaler_a_b in [layer.attn.q_scaler, layer.attn.k_scaler, layer.attn.v_scaler, layer.attn.att_score_scaler, layer.attn.NL_scaler, layer.attn.output_scaler]:\n",
    "            scaler_a_b.calibration = False\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to optimize through gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "warmup_iters = 20\n",
    "min_lr = 0.001\n",
    "learning_rate = 0.1\n",
    "lr_decay_iters = 100\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + np.cos(pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # for layer in model.transformer.h:\n",
    "    for layer in model.transformer.h:\n",
    "        for scaler_a_b in [layer.attn.q_scaler, layer.attn.k_scaler, layer.attn.v_scaler, layer.attn.att_score_scaler, layer.attn.NL_scaler, layer.attn.output_scaler]:\n",
    "            scaler_a_b.calibration = True\n",
    "       \n",
    "    model_linear = model_linear.to(device)\n",
    "    model_linear.train()\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    done = False\n",
    "    error_threshold = 0.01\n",
    "    calibration_iter = 0\n",
    "    max_calibration_iter = 10000\n",
    "    calib_lr = 0.1\n",
    "    \n",
    "    parameters = []\n",
    "    # for layer in model.transformer.h:\n",
    "    for layer in model.transformer.h:\n",
    "        parameters += list(layer.attn.q_scaler.parameters())\n",
    "        parameters += list(layer.attn.k_scaler.parameters())\n",
    "        parameters += list(layer.attn.v_scaler.parameters())\n",
    "        parameters += list(layer.attn.att_score_scaler.parameters())\n",
    "        parameters += list(layer.attn.NL_scaler.parameters())\n",
    "        parameters += list(layer.attn.output_scaler.parameters())\n",
    "        \n",
    "    optim = torch.optim.AdamW(params=parameters, lr=calib_lr)\n",
    "    \n",
    "    std_errors_plots = torch.zeros(6 * config_args['n_layer'], 0)\n",
    "    mean_errors_plots = torch.zeros(6 * config_args['n_layer'], 0)\n",
    "    total_loss = torch.zeros(0)\n",
    "    while not(done):\n",
    "        X, Y = get_batch('train')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits_linear, _ = model_linear(X, Y)  \n",
    "        logits, _ = model(X, Y)\n",
    "        # for param_group in optim.param_groups:\n",
    "        #     param_group['lr'] = get_lr(calibration_iter)      \n",
    "        optim.zero_grad()\n",
    "        loss = 0.\n",
    "        # for (layer, linear_layer) in zip(model.transformer.h, model_linear.transformer.h):\n",
    "        #     loss += nn.L1Loss()(layer.attn.q_scaler.output, linear_layer.attn.q_scaler.output)\n",
    "        #     loss += nn.L1Loss()(layer.attn.k_scaler.output, linear_layer.attn.k_scaler.output)\n",
    "        #     loss += nn.L1Loss()(layer.attn.v_scaler.output, linear_layer.attn.v_scaler.output)\n",
    "        #     loss += nn.L1Loss()(layer.attn.att_score_scaler.output, linear_layer.attn.att_score_scaler.output)\n",
    "        #     loss += nn.L1Loss()(layer.attn.NL_scaler.output, linear_layer.attn.NL_scaler.output)\n",
    "        #     loss += nn.L1Loss()(layer.attn.output_scaler.output, linear_layer.attn.output_scaler.output)\n",
    "            \n",
    "        for (layer, linear_layer) in zip(model.transformer.h, model_linear.transformer.h):\n",
    "            loss += nn.L1Loss()(layer.attn.q_scaler.std_after_scale, linear_layer.attn.q_scaler.std_after_scale)\n",
    "            loss += nn.L1Loss()(layer.attn.k_scaler.std_after_scale, linear_layer.attn.k_scaler.std_after_scale)\n",
    "            loss += nn.L1Loss()(layer.attn.v_scaler.std_after_scale, linear_layer.attn.v_scaler.std_after_scale)\n",
    "            loss += nn.L1Loss()(layer.attn.att_score_scaler.std_after_scale, linear_layer.attn.att_score_scaler.std_after_scale)\n",
    "            loss += nn.L1Loss()(layer.attn.NL_scaler.std_after_scale, linear_layer.attn.NL_scaler.std_after_scale)\n",
    "            loss += nn.L1Loss()(layer.attn.output_scaler.std_after_scale, linear_layer.attn.output_scaler.std_after_scale) \n",
    "            \n",
    "            loss += nn.L1Loss()(layer.attn.q_scaler.mean_after_scale, linear_layer.attn.q_scaler.mean_after_scale)\n",
    "            loss += nn.L1Loss()(layer.attn.k_scaler.mean_after_scale, linear_layer.attn.k_scaler.mean_after_scale)\n",
    "            loss += nn.L1Loss()(layer.attn.v_scaler.mean_after_scale, linear_layer.attn.v_scaler.mean_after_scale)\n",
    "            loss += nn.L1Loss()(layer.attn.att_score_scaler.mean_after_scale, linear_layer.attn.att_score_scaler.mean_after_scale)\n",
    "            loss += nn.L1Loss()(layer.attn.NL_scaler.mean_after_scale, linear_layer.attn.NL_scaler.mean_after_scale)\n",
    "            loss += nn.L1Loss()(layer.attn.output_scaler.mean_after_scale, linear_layer.attn.output_scaler.mean_after_scale)    \n",
    "        \n",
    "        total_loss = torch.cat((total_loss, loss.detach().cpu().unsqueeze(0)))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            std_errors = []\n",
    "            mean_errors = []\n",
    "            done_list = []\n",
    "            \n",
    "            # for layer in model.transformer.h:\n",
    "            for (layer, linear_layer) in zip(model.transformer.h, model_linear.transformer.h):\n",
    "                for (scaler_a_b, linear_scaler_a_b) in zip([layer.attn.q_scaler, layer.attn.k_scaler, layer.attn.v_scaler, layer.attn.att_score_scaler, layer.attn.NL_scaler, layer.attn.output_scaler], [linear_layer.attn.q_scaler, linear_layer.attn.k_scaler, linear_layer.attn.v_scaler, linear_layer.attn.att_score_scaler, linear_layer.attn.NL_scaler, linear_layer.attn.output_scaler]):                \n",
    "                    std_errors += [torch.abs(scaler_a_b.std_after_scale-linear_scaler_a_b.std_after_scale).item()]\n",
    "                    mean_errors += [torch.abs(scaler_a_b.mean_after_scale-linear_scaler_a_b.mean_after_scale).item()]\n",
    "                    if std_errors[-1] > error_threshold:\n",
    "                        done_list += [False]\n",
    "                    else:\n",
    "                        done_list += [True]\n",
    "                    if mean_errors[-1] > error_threshold:\n",
    "                        done_list += [False]\n",
    "                    else:\n",
    "                        done_list += [True]\n",
    "            std_errors_plots = torch.cat((std_errors_plots, torch.tensor(std_errors).unsqueeze(1)), dim=-1)\n",
    "            mean_errors_plots = torch.cat((mean_errors_plots, torch.tensor(mean_errors).unsqueeze(1)), dim=-1)\n",
    "            if calibration_iter % 10 == 0:\n",
    "                fig, ax = plt.subplots(1,2)\n",
    "                clear_output(wait=True)\n",
    "                for param in range(len(std_errors_plots)):\n",
    "                    ax[0].plot(std_errors_plots[param], linewidth=0.5)\n",
    "                    ax[0].plot(mean_errors_plots[param], linewidth=0.5)\n",
    "                    ax[0].set_yscale('log')\n",
    "                ax[0].hlines(error_threshold, 0., len(std_errors_plots[param])-1.0, colors='black', linewidth=2)\n",
    "                ax[1].plot(total_loss, linewidth=2)\n",
    "                ax[1].set_ylabel('Loss')\n",
    "                ax[1].set_yscale('log')\n",
    "                fig.tight_layout()\n",
    "                plt.show()\n",
    "                sys.stdout.flush()\n",
    "                plt.pause(0.1)  # Pause to update the plot\n",
    "            \n",
    "            calibration_iter += 1\n",
    "            \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        print(f'Calibraton iter {calibration_iter}| Loss: {loss.item():.3f} | error threshold: {error_threshold:.3f}\\tnum valid params: {torch.sum(torch.tensor(done_list))}/{len(done_list)}\\tstd errors: {torch.sort(torch.tensor(std_errors), descending=True)[0][:3]}\\tmean errors: {torch.sort(torch.tensor(mean_errors), descending=True)[0][:3]}')\n",
    "        print(model.transformer.h[0].attn.att_score_scaler.a.item())\n",
    "        \n",
    "        assert calibration_iter < max_calibration_iter, f'Calibration algorithm did not converge after {calibration_iter} steps.'\n",
    "        if torch.all(torch.tensor(done_list)):\n",
    "            done = True                     \n",
    "    # End calibration procedure\n",
    "    # for layer in model.transformer.h:\n",
    "    for layer in model.transformer.h:\n",
    "        for scaler_a_b in [layer.attn.q_scaler, layer.attn.k_scaler, layer.attn.v_scaler, layer.attn.att_score_scaler, layer.attn.NL_scaler, layer.attn.output_scaler]:\n",
    "            scaler_a_b.calibration = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
