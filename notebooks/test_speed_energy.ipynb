{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "dir_name = os.getcwd()\n",
    "parent_dir_name = os.path.dirname(dir_name)\n",
    "sys.path.insert(0, parent_dir_name)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pynvml\n",
    "from pynvml.smi import nvidia_smi\n",
    "import sys\n",
    "# import asyncio\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "'''\n",
    "How to measure GPU consumption\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0) <- for GPU 0\n",
    "power = pynvml.nvmlDeviceGetPowerUsage(handle) <- returns power in mW\n",
    "'''\n",
    "S = 256\n",
    "H = 12\n",
    "D = 1024\n",
    "D_h = 64\n",
    "# T_tensor = torch.tensor([1024, 1024*2, 1024*4, 1024*8])\n",
    "T_tensor = torch.tensor([1024])\n",
    "\n",
    "sequential_model = True\n",
    "if sequential_model:\n",
    "    offset_step = 10\n",
    "    n_steps = 10\n",
    "else:\n",
    "    offset_step = 100\n",
    "    n_steps = 1000\n",
    "    \n",
    "device = 0\n",
    "# qkv_proj_nmul = S * D * D_h * 3 * H * T\n",
    "# dot_product_nmul = S * T * T * D_h * H    # should change to M * T * D * H depending wether we want to compare with self attention or sliding-window attention\n",
    "# v_prod_nmul = S * T * T * D_h * H         # should change to M * T * D * H depending wether we want to compare with self attention or sliding-window attention\n",
    "# out_proj_nmul = S * H * D_h * D * T\n",
    "# total_nmul = qkv_proj_nmul + dot_product_nmul + v_prod_nmul + out_proj_nmul\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(device)\n",
    "mW_to_W = 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, block_size=1024, window_size=4096):\n",
    "        super().__init__()\n",
    "        if block_size > window_size:\n",
    "            block_size = window_size\n",
    "        self.window_size = window_size\n",
    "        self.register_buffer(\"k_cache\", torch.zeros(S, block_size, H, D_h), persistent=False)\n",
    "        self.register_buffer(\"v_cache\", torch.zeros(S, block_size, H, D_h), persistent=False)\n",
    "        self.flash_attention = False\n",
    "        self.kv_cache = True        \n",
    "    def forward(self, x, t, qkv=None):\n",
    "        q, k, v = qkv # (S, T, H, D_h)\n",
    "        n_samples, seq_len, _, _ = q.shape\n",
    "        if self.kv_cache:\n",
    "            t = t % self.window_size # sliding window\n",
    "            self.k_cache[:n_samples, t:t+seq_len] = k\n",
    "            self.v_cache[:n_samples, t:t+seq_len] = v\n",
    "            k = self.k_cache\n",
    "            v = self.v_cache\n",
    "            q = q.transpose(1,2)\n",
    "            k = k.transpose(1,2)\n",
    "            v = v.transpose(1,2)\n",
    "        if self.flash_attention:\n",
    "            nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        else:\n",
    "            k = k.transpose(-1, -2)\n",
    "            x = torch.matmul(q, k)\n",
    "            x = torch.matmul(x, v)   \n",
    "    \n",
    "mean_latency_per_token = torch.zeros(len(T_tensor))\n",
    "mean_energy_per_token_per_head = torch.zeros(len(T_tensor))\n",
    "\n",
    "for block_size_idx, T in enumerate(T_tensor):\n",
    "    net = Attention(block_size=T).to(device)\n",
    "    x = torch.rand(S, T, D).to(device)\n",
    "    qkv = torch.rand(3, S, T, H, D_h).to(device)\n",
    "    latency_tensor = torch.zeros(n_steps)\n",
    "    power_tensor = torch.zeros(n_steps)\n",
    "    energy_tensor = torch.zeros(n_steps)\n",
    "    # energy_per_multiplication = torch.zeros(n_steps)\n",
    "    with torch.no_grad():\n",
    "        # warmup the gpus\n",
    "        for i in range(offset_step):\n",
    "            if sequential_model:    \n",
    "                for t in range(T):\n",
    "                    net(x, t=t, qkv=qkv[:,:,t].unsqueeze(2))  \n",
    "            else:\n",
    "                net(x, t=0, qkv=qkv)\n",
    "   \n",
    "            print(f'Offset step {i+1}/{offset_step}', end='\\r')  \n",
    "        \n",
    "        start = time.time()\n",
    "        for i in range(n_steps):\n",
    "            inner_start = time.time()\n",
    "            if sequential_model:  \n",
    "                for t in range(T):\n",
    "                    net(x, t=t, qkv=qkv[:,:,t].unsqueeze(2))  \n",
    "            else:\n",
    "                net(x, t=0, qkv=qkv)\n",
    "      \n",
    "            inner_end = time.time()\n",
    "            power = pynvml.nvmlDeviceGetPowerUsage(handle) / mW_to_W\n",
    "            memory_usage = nvidia_smi.getInstance().DeviceQuery('memory.used')['gpu'][device]['fb_memory_usage']['used']\n",
    "            latency_tensor[i] = inner_end - inner_start\n",
    "            power_tensor[i] = power\n",
    "            energy_tensor[i] = power_tensor[i] * latency_tensor[i]\n",
    "            # energy_per_multiplication[i] = energy_tensor[i] / (dot_product_nmul + v_prod_nmul)\n",
    "            if i % 1==0:\n",
    "                print('GPU power consumption:', f'{power_tensor[i].item():.2f} W | ',\n",
    "                    # 'Energy per multiplication:', f'{energy_per_multiplication[i].item():.2e} J | ',\n",
    "                    'Memory usage:', f'{memory_usage:.2f} Mb | ',\n",
    "                    f'Step {i+1}/{n_steps}', end='\\r',\n",
    "                    )\n",
    "    end = time.time()\n",
    "    time_to_end = end - start\n",
    "    mean_power = power_tensor.mean()\n",
    "    mean_latency = latency_tensor.mean()\n",
    "    mean_energy = mean_power * mean_latency\n",
    "\n",
    "    mean_latency_per_token[block_size_idx] = mean_latency / S / T\n",
    "    mean_energy_per_token_per_head[block_size_idx] = mean_energy / H / S / T\n",
    "    \n",
    "    print(f'\\nblock size: {T}|\\tmean latency per token: {mean_latency_per_token[block_size_idx].item():.2e}|\\tmean energy per token per head: {mean_energy_per_token_per_head[block_size_idx].item():.2e}')\n",
    "    \n",
    "print(f'Block sizes: {T_tensor}')\n",
    "print(f'Mean latency per token: {mean_latency_per_token}')\n",
    "print(f'Mean energy per token per head: {mean_energy_per_token_per_head}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# latency_ = [mean_latency_per_token]\n",
    "# energy_ = [mean_energy_per_token_per_head]\n",
    "\n",
    "import torch\n",
    "\n",
    "# With mistral attention size\n",
    "T_tensor = torch.tensor([1024, 1024*2, 1024*4, 1024*8])\n",
    "latency_ = [torch.tensor([3.3538e-07, 8.3981e-07, 1.7357e-06, 3.5858e-06]),\n",
    "            # torch.tensor([8.4214e-05, 2.1825e-04, 5.7352e-04, 0.0006]),, # not actually sliding window\n",
    "            torch.tensor([8.4214e-05, 2.1825e-04, 5.7352e-04, 1.4094e-03]), # actually sliding window -> the energy saturates for tokens > 4096\n",
    "                         ]\n",
    "energy_ = [torch.tensor([1.9843e-06, 1.1707e-05, 2.4183e-05, 4.9717e-05]),\n",
    "        #    torch.tensor([0.0008, 0.0022, 0.0058, 0.0130]), # not actually sliding window\n",
    "           torch.tensor([0.0008, 0.0022, 0.0058, 0.0058]), # actually sliding window -> the energy saturates for tokens > 4096\n",
    "                         ]\n",
    "\n",
    "# With gpt2 attention size\n",
    "\n",
    "label_ = ['Parallel FlashAttention',\n",
    "          'Sequential FlashAttention',\n",
    "          ]\n",
    "\n",
    "marker_color=[{'marker':'-s', 'color': 'black'},\n",
    "              {'marker':'-o', 'color': 'black'},\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter, NullFormatter, ScalarFormatter\n",
    "from matplotlib import rc, rcParams\n",
    "\n",
    "font = {'size': 8}\n",
    "rc('font', **font)\n",
    "rcParams['mathtext.default'] = 'regular'  # Math subscripts and greek letters non-italic\n",
    "linewidth = 2\n",
    "marker_size = 5\n",
    "centimeters = 1 / 2.54  # centimeters in inches\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_figwidth(15*centimeters)\n",
    "fig.set_figheight(6*centimeters)\n",
    "save_plot = True\n",
    "file_out = '../plots/energ_latency_dram'\n",
    "\n",
    "for exp, (mean_latency_per_token, mean_energy_per_token_per_head, label) in enumerate(zip(latency_, energy_, label_)):\n",
    "    ax[0].plot(T_tensor, mean_latency_per_token*1e+6, marker_color[exp]['marker'], color=marker_color[exp]['color'], label=label, linewidth=linewidth, ms=marker_size)\n",
    "    ax[0].set_xlabel('# Tokens', fontsize=font['size'])\n",
    "    ax[0].set_ylabel('Latency per token (µs)', fontsize=font['size'])\n",
    "    ax[0].set_xscale('log', base=2)\n",
    "    ax[0].set_yscale('log', base=10)\n",
    "    # ax[0].set_xticks(list(T_tensor.numpy()))\n",
    "    # ax[0].set_xticks([1000, 2000, 4000, 6000])\n",
    "    ax[0].xaxis.set_major_formatter(StrMethodFormatter('{x:.0f}'))\n",
    "    # ax[0].yaxis.set_major_formatter(StrMethodFormatter('{x:.0e}'))\n",
    "\n",
    "    ax[1].plot(T_tensor, mean_energy_per_token_per_head*1e+6, marker_color[exp]['marker'], color=marker_color[exp]['color'], label=label, linewidth=linewidth, ms=marker_size)\n",
    "    ax[1].set_xlabel('# Tokens', fontsize=font['size'])\n",
    "    ax[1].set_ylabel('Energy/tokens/heads (µJ)', fontsize=font['size'])\n",
    "    ax[1].set_xscale('log', base=2)\n",
    "    ax[1].set_yscale('log', base=10)\n",
    "    # ax[1].set_xticks(list(T_tensor.numpy()))\n",
    "    # ax[1].set_xticks([1000, 2000, 4000, 6000])\n",
    "    ax[1].xaxis.set_major_formatter(StrMethodFormatter('{x:.0f}'))\n",
    "    # ax[1].yaxis.set_major_formatter(StrMethodFormatter('{x:.0e}'))\n",
    "\n",
    "ax[0].legend(frameon=False)\n",
    "ax[1].legend(frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "ax[0].xaxis.labelpad = 5\n",
    "ax[0].yaxis.labelpad = 5\n",
    "ax[1].xaxis.labelpad = 5\n",
    "ax[1].yaxis.labelpad = 5\n",
    "\n",
    "if save_plot:    \n",
    "    for fmt in ['png', 'svg', 'pdf']:\n",
    "        plt.savefig(file_out + '.%s' % fmt, format=fmt, dpi=1200)    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot latency and power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig, ax = plt.subplots(3, 2)\n",
    "\n",
    "# ax[0,0].plot(latency_tensor, 'darkblue', linewidth=1)\n",
    "# ax[0,0].set_ylabel('Latency (s)')\n",
    "# ax[1,0].plot(power_tensor, 'darkred', linewidth=1)\n",
    "# ax[1,0].set_ylabel('Power (W)')\n",
    "# ax[2,0].plot(energy_tensor, 'darkgreen', linewidth=1)\n",
    "# ax[2,0].set_ylabel('Energy (J)')\n",
    "# ax[2,0].set_xlabel('Repetitions')\n",
    "\n",
    "# ax[0,1].plot(latency_tensor / T / S, 'darkblue', linewidth=1)\n",
    "# ax[0,1].set_ylabel('Latency per token (s)')\n",
    "# ax[1,1].plot(power_tensor / H / S / T, 'darkred', linewidth=1)\n",
    "# ax[1,1].set_ylabel('Power per head\\nper token (W)')\n",
    "# ax[2,1].plot(energy_tensor / H / S / T, 'darkgreen', linewidth=1)\n",
    "# ax[2,1].set_ylabel('Energy per head\\nper token (J)')\n",
    "# ax[2,1].set_xlabel('Repetitions')\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
